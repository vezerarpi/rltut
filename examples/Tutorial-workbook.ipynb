{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning Tutorial Workbook\n",
    "\n",
    "An introduction to reinforcement learning that creates an agent that learns to play the CartPole environment in OpenAI Gym using a slightly simplified version of the DQN algorithm.\n",
    "\n",
    "See the `Guide.ipynb` notebook for a brief introduction to the theory behind Q-learning.\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "Dependencies and setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import chainer as C\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import model, log\n",
    "\n",
    "\n",
    "# create a random number generator and seed it so runs are repeatable\n",
    "rng = random.Random()\n",
    "rng.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Gym\n",
    "\n",
    "OpenAI Gym wraps a range of simple RL environments in a fairly easy to use module. We will start with the most basic one, CartPole, which has a simple enough state space to be easily visualised but still has some interesting dynamics for an agent to learn.\n",
    "\n",
    "The `gym.make()` function creates environment by name, in this case `'CartPole-v0'`. Once an environment has been created we can query it for its observation and action spaces and instruct actions to be taken in it to get a reward and the next state. The environment has to be reset to start get an initial state and start what is called an episode. An episode is a series of steps in the environment, ended when a terminal state is reached or we next call reset (it often makes sense to have a max number of steps even if a terminal state hasn't been reached yet).\n",
    "\n",
    "* `state = env.make(name)` creates an environment\n",
    "* `state = env.reset()` resets the environment, starts a new episode and returns an initial state observation\n",
    "* `next_state, reward, done = env.step(action)` performs the action in the environment and returns the next state, the reward for the action and whether current episode has ended\n",
    "* The action and observation spaces have a `sample()` method to sample random states and examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "print('observation space', env.observation_space)\n",
    "print('action space', env.action_space, 'action_space.n', env.action_space.n)\n",
    "state = env.reset()\n",
    "print('state', state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole\n",
    "\n",
    "The task in CartPole is to keep upright a pole that is balanced on a cart by moving the cart either left or right. The episode ends if the pole swings past 12 degrees from upright or if the cart moves out of bounds.\n",
    "\n",
    "The environment rewards a value of 1 for each time step taken, including on the final timestep that ends the episode. The goal is therefore to keep the pole within bounds for a long as possible, up to 200 steps.\n",
    "\n",
    "We generally don't need to know the exact details of the state or the action spaces and just need to know the sizes so that an RL algorithm can know what input it takes and how many actions to select from. The state for cartpole is a quadruple: `[cart_position, cart_velocity, pole_theta, pole_angular_velocity]`. The pole angle and velocity is in radians, vertical is 0.0, left of centre is negative. For an Atari game we would just have a 3D matrix containing the RGB values of the current state of the screen, H x W x (R, G, B), but more on that later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(5):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    reward = 0.0\n",
    "    actions = list(range(env.action_space.n))\n",
    "    steps = 0\n",
    "    while not done:\n",
    "        state, r, done, _ = env.step(random.choice(actions))\n",
    "        reward += r\n",
    "        steps += 1\n",
    "    print(ep, 'steps', steps, 'reward', reward, 'final state', state)\n",
    "    print('-' * 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging and video output\n",
    "\n",
    "Gym can output videos of episodes, which for CartPole can be played in real time. This is useful for seeing how well an algorithm has learnt to play the game. This is easily done by wrapping the environment in a `gym.wrappers.Monitor`, which has the following constructor arguments:\n",
    "* `directory` - output directory path, make sure you've personalised it above\n",
    "* `force` - flag to force creation of new, or overwriting of existsing output directires\n",
    "* `video_callable` - a function that takes the episode number and returns whther to record a video, defaults to every cubic number or every 1K episodes after the first 1K\n",
    "\n",
    "The `log` module has a Monitor class wrapping gym's Monitor to provide extra logging at the end of each episode as well as recording videos, both of which can be viewed in the `Log.ipynb` notebook. The `log.Monitor` class accepts all the arguments mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = log.Monitor(env, directory=log.create_directory(), print_every=1,\n",
    "                  force=True, video_callable=lambda ep: ep % 10 == 0)\n",
    "# TODO Try the previous random actions here again but now with the monitor wrapping the env\n",
    "# TODO and view the output by opening  and running the Log notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "\n",
    "See the `Guide.ipynb` notebook for more details on the action-value Q function and the loss used to train an agent to estimate it. We will implement the following algorithm to sample batches of experiences (transitions from the current state to a next state following an action) and compute a loss to minimise that will lead the model to learning better and better approximations of expected rewards of the possible actions at any given state.\n",
    "\n",
    "```\n",
    "Initialise replay buffer D\n",
    "Initialise Q function with random weights w\n",
    "for episode = 1, M:\n",
    "    Initialise the environment and the initial state\n",
    "    for t = 1, T:\n",
    "        With probability eps selct a random action a_t\n",
    "        otherwise select a_t = argmax(Q(state)) over possible actions a\n",
    "        Execute action a_t in the environment and observe next_state, r_t, done\n",
    "        Store (state, a_t, r_t, next_state, done) in D\n",
    "        Sample a batch_size batch of experiences (s_j, a_j, r_j, s_j+1, done) from D\n",
    "        Set loss = 0\n",
    "        For each experience in batch:\n",
    "            set y = r_j if done else\n",
    "                    r_j + gamma * max(Q(s_j+1))\n",
    "            loss += (y_j - Q(s_j)[a_j])**2\n",
    "        loss /= batch_size\n",
    "        Perform a gradient update step on the loss wrt the weights w\n",
    "        if done: break\n",
    "```\n",
    "\n",
    "## Agent\n",
    "\n",
    "We will implement the algorithm by factoring most of the logic and calculations around states, rewqards and the loss to an `Agent` class. This class will manage the selection of actions, tracking the current experience and the updates of the parameters of Q-function model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Create an agent containing\n",
    "         * a model\n",
    "         * an optimiser\n",
    "         * an experience buffer\n",
    "        '''\n",
    "        self._lr = 0.01\n",
    "        self._batch_size = 64\n",
    "        self._gamma = 0.95\n",
    "        self._epsilon = 0.1\n",
    "        self._model = model.CartPoleModel()\n",
    "        # This sets up a chainer optimiser that will be used to backprop the\n",
    "        # loss through the model\n",
    "        self._optim = C.optimizers.SGD(lr=self._lr)\n",
    "        self._optim.setup(self._model)\n",
    "        # TODO create a member for the experience replay buffer (just store in a\n",
    "        # list).\n",
    "\n",
    "    def act(self, state):\n",
    "        '''\n",
    "        Returns an action (as an int) for the current state. Keeps track of the state\n",
    "        and the action taken for use in the next call to reward(). The action is\n",
    "        chosen e-greedily (a random action is taken with probability e).\n",
    "\n",
    "         - state a np.array representing the current observed state\n",
    "         - return the selected action\n",
    "        '''\n",
    "        state = state.astype(np.float32)\n",
    "\n",
    "        # Wrap the state in a chainer.Variable ready to be passed as an argument\n",
    "        # to the model. This is how data must be fed in to Chainer models. The\n",
    "        # value of a Variable can be accessed as a numpy array using its .data\n",
    "        # property. The state is reshaped to have an outer array dimension as\n",
    "        # chainer requires all inputs to have a batch dimension. In this case we\n",
    "        # have a batch of 1 x state with shape (1, 4).\n",
    "        model_input = C.Variable(state.reshape((1, -1)))\n",
    "\n",
    "        # TODO Store the state that was passed in\n",
    "\n",
    "        # TODO Select and store an action using self._model or choosing a random\n",
    "        # action with a small probability (self._epsilon). Return the action.\n",
    "\n",
    "    def reward(self, reward, next_state, done):\n",
    "        '''\n",
    "        Takes the reward for the last action and the resulting next_state,\n",
    "        calculates the Q-learning loss and performs a parameter update on the\n",
    "        model for a miniubatch sampled from the experience buffer.\n",
    "\n",
    "          - reward a float, the reward for the latest act()\n",
    "          - next_state a np.array containing the observed next state resulting\n",
    "         from the latest act()\n",
    "          - done a bool indicating whether the next state is a terminal state\n",
    "          - return The average loss for the latest batch\n",
    "        '''\n",
    "        # TODO Append the latest experience to the replay buffer. The experience\n",
    "        # should contain (current state, action, reward, next state, done),\n",
    "        # where current state and action should have been stored by the agent on\n",
    "        # the last call to act(). It might help to store each experience as a\n",
    "        # dict so that the lookups from it are easily readable.\n",
    "\n",
    "        # TODO Sample a batch (self._batch_size) from your replay buffer. See\n",
    "        # https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.choice.html\n",
    "\n",
    "        # TODO Evaluate the next states in the sampled batch. These can be done\n",
    "        # in a loop or all at once if you use np.stack to concatenate all you\n",
    "        # next states along an outer batch dimension. Remeber that a batch\n",
    "        # dimension is always needed by the model if evaluating the states\n",
    "        # individually.\n",
    "        y = None # TODO the output of the model on the next state(s)\n",
    "\n",
    "        # Importantly, the Q value results for the next states must be converted\n",
    "        # back to a numpy array before being used in subsequent calculations.\n",
    "        # This is because we do not want gradients to be applied as a result of\n",
    "        # the previous calc, as we want the loss to move the Q values for the\n",
    "        # current state closer to the values of the next states, leaving hte\n",
    "        # next state values unchanged. Converting to numpy breaks the chain of\n",
    "        # gradient bookkeeping in Chainer.\n",
    "        y = y.data\n",
    "\n",
    "        # This is required to make sure only the current calculations' gradients\n",
    "        # are used in the update when we call loss.backward(). All loss\n",
    "        # calculations, except the y values mentioned above need to come AFTER\n",
    "        # this cleargrads() call.\n",
    "        self._model.cleargrads()\n",
    "\n",
    "        # Calculate the loss.\n",
    "\n",
    "        # TODO Get the q values for the current states in the sampled batch from\n",
    "        # the model, keep these as Chainer Variables by making sure you do not\n",
    "        # call data on them and use Chainer operations to implement the loss.\n",
    "        # The q values for the actual actions that were taken should be used,\n",
    "        # these can be selected using C.functions.select_item:\n",
    "        # https://docs.chainer.org/en/stable/reference/generated/chainer.functions.select_item.html\n",
    "\n",
    "        # TODO With the Q values for all actions at each state in the batch\n",
    "        # calculated, we need to select the Qs for the actual action that was\n",
    "        # taken for each state, according to the sampled experiences. The\n",
    "        # Chainer Variable that holds these values can be indexed like a numpy\n",
    "        # array, or the actions can be selected individually and combined into a\n",
    "        # batch-shaped Variable using C.functions.stack. See\n",
    "        # http://docs.chainer.org/en/stable/reference/generated/chainer.functions.stack.html\n",
    "\n",
    "        # TODO Calculate the loss using chainer functions. Remember to take the\n",
    "        # done flag for each element of the batch into account.\n",
    "        # See C.functions.mean_squared_error\n",
    "        # http://docs.chainer.org/en/stable/reference/generated/chainer.functions.mean_squared_error.html\n",
    "        loss = None # TODO see the pseudo code above or the guide for the loss\n",
    "\n",
    "        # These two lines compute the gradient fo the calculations that we just\n",
    "        # did and update the model using the optimiser\n",
    "        loss.backward()\n",
    "        self._optim.update()\n",
    "\n",
    "        # The average loss for the batch is returned\n",
    "        return np.asscalar(loss.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "Now that we have an agent we can start training it over multiple episodes of the environment.\n",
    "\n",
    "It might be useful to print out an evaluation of the model for a fixed set of states to be able to check that the values are changing and that it is beginning to behave as expected in those states.\n",
    "\n",
    "After a certain number of episodes you can use the `CartPoleModel.print_eval` method to print the Q values form the model for 7 pole angles between +/-10 degrees. These Q values should increase over time as your model experiences longer episodes , and thus more rewards, when it chooses actions that help stabilise the pole. If your values are not changing from one iteration to the next then there may be a problem with the chainer code that is stopping gradient updates s from being applied back through the components of the model. These outputs are only a rough indicator of how well your model is doing. It is unlikely to ever see exactly these states so it may not have made the best decision for each of them, but if the rewards are not increasing and the Q values do not look like they are moving towards values that you would expect given the pole orientation then something is probably wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 1000\n",
    "# Re-initialise the Environment and Monitor\n",
    "env = gym.make('CartPole-v0')\n",
    "env = log.Monitor(env, directory=log.create_directory(),\n",
    "                  print_every=10, video_callable=lambda ep: ep % 20 == 0)\n",
    "env.seed(0)\n",
    "agent = Agent()\n",
    "\n",
    "for ep in range(n_episodes):\n",
    "    # TODO step through the environment until done, using agent.act() and\n",
    "    # agent.reward()\n",
    "\n",
    "# TODO print the total reward after each episode, optionally also call\n",
    "# the model's print_eval() to see how the agent is learnign to estimate\n",
    "# those particular states every now and then (e.g. every 20 episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further work\n",
    "\n",
    "Create a new agent that uses the `model.AtariModel` to learn to play the game 'Breakout-v4'. This is an Atari game environment whose observations are the RGB pixels of the screen and has the following actions: 0: do nothing, 1: move left, 2: request ball, 3: move right.\n",
    "\n",
    "This task is a lot more computationally intensive so the image should be scaled down to a quater of its size before being passed to the model. Even with this it will take hours before it starts to score a few points but you should be able to see improvements in the first couple of hundred episodes. Once you are confident that the agent is beginning to learn to play then you could start modifying you it to execute the model calculations and updates on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
